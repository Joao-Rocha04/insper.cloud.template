{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-l","title":"KIT-L","text":"<p>Jo\u00e3o Vitor Rocha</p> <p>Marinna Grigolli Cesar</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 14/03/2025</li> <li> Roteiro 2 - Data 14/04/2025</li> <li> Roteiro 3 - Data 17/05/2025</li> <li> Roteiro 4 - Data 26/05/2025</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto","text":""},{"location":"projeto/main/#projeto-api-cloud-computacao-em-nuvem-20251","title":"Projeto API Cloud \u2013 Computa\u00e7\u00e3o em Nuvem (2025.1)","text":"<p>Este projeto desenvolve uma API RESTful com as seguintes funcionalidades:</p> <ul> <li>Registro de usu\u00e1rios</li> <li>Autentica\u00e7\u00e3o via JWT</li> <li>Consulta de dados externos (via scraping)</li> <li>Verifica\u00e7\u00e3o de sa\u00fade da aplica\u00e7\u00e3o (<code>/health-check</code>)</li> </ul> <p>A aplica\u00e7\u00e3o foi desenvolvida com FastAPI, containerizada com Docker e implantada na nuvem via AWS Lightsail.</p>"},{"location":"projeto/main/#tecnologias-utilizadas","title":"Tecnologias utilizadas","text":"<ul> <li>FastAPI (framework da API)</li> <li>PostgreSQL (banco de dados relacional)</li> <li>Docker (containeriza\u00e7\u00e3o)</li> <li>JWT (autentica\u00e7\u00e3o segura)</li> <li>AWS Lightsail (deploy e banco gerenciado)</li> </ul>"},{"location":"projeto/main/#como-executar-a-aplicacao-localmente","title":"Como executar a aplica\u00e7\u00e3o localmente","text":"<p>A API est\u00e1 dispon\u00edvel como uma imagem Docker p\u00fablica.</p>"},{"location":"projeto/main/#requisitos","title":"Requisitos","text":"<ul> <li>Docker instalado na m\u00e1quina</li> </ul>"},{"location":"projeto/main/#execucao-rapida","title":"Execu\u00e7\u00e3o r\u00e1pida","text":"<p>Voc\u00ea pode rodar a API diretamente com o seguinte comando:</p> <p>docker run -p 8080:8080 marinnagc/api_cloud:latest</p> <p>Acesse: http://localhost:8080/docs</p> <p>A API estar\u00e1 rodando localmente, e ser\u00e1 poss\u00edvel interagir com os endpoints via Swagger.</p>"},{"location":"projeto/main/#docker-hub","title":"Docker Hub","text":"<p>A imagem da API foi publicada no Docker Hub:</p> <p>\ud83d\udd17 marinnagc/api_cloud:latest</p>"},{"location":"projeto/main/#deploy-na-nuvem-aws-lightsail","title":"Deploy na Nuvem \u2013 AWS Lightsail","text":"<p>A aplica\u00e7\u00e3o est\u00e1 implantada em:</p> <p>\ud83d\udd17 https://container-service-1.0wfm340q3x1pc.us-east-1.cs.amazonlightsail.com/docs</p>"},{"location":"projeto/main/#configuracoes-da-aws","title":"Configura\u00e7\u00f5es da AWS:","text":"<ul> <li>Container: Plano Nano (512MB RAM, 0.25 vCPU)</li> <li>Banco de Dados: PostgreSQL gerenciado (plano b\u00e1sico)</li> <li>Conex\u00e3o: Feita por vari\u00e1veis de ambiente (no compose)</li> </ul>"},{"location":"projeto/main/#endpoints-da-api","title":"Endpoints da API","text":""},{"location":"projeto/main/#post-registrar","title":"POST <code>/registrar</code>","text":"<p>Registra um novo usu\u00e1rio.</p> <p>Request:</p> <pre><code>{\n  \"nome\": \"Maria\",\n  \"email\": \"maria@insper.edu.br\",\n  \"senha\": \"senha123\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"token\": \"&lt;TOKEN&gt;&gt;\"\n}\n</code></pre>"},{"location":"projeto/main/#post-login","title":"POST <code>/login</code>","text":"<p>Autentica o usu\u00e1rio.</p> <p>Request:</p> <pre><code>{\n  \"email\": \"maria@insper.edu.br\",\n  \"senha\": \"senha123\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"token\": \"&lt;TOKEN&gt;&gt;\"\n}\n</code></pre>"},{"location":"projeto/main/#get-consultar","title":"GET <code>/consultar</code>","text":"<p>Consulta dados externos (scraping). Requer header: <code>Authorization: Bearer &lt;token&gt;</code></p> <p>Response exemplo:</p> <pre><code>{\n  \"cidade\": \"S\u00e3o Paulo\",\n  \"temperatura\": \"16.0\u00b0C\",\n  \"condicao\": \"Parcialmente nublado\"\n}\n</code></pre>"},{"location":"projeto/main/#get-health-check","title":"GET <code>/health-check</code>","text":"<p>Verifica se a aplica\u00e7\u00e3o est\u00e1 no ar.</p> <p>Response:</p> <pre><code>{\n  \"statusCode\": 200,\n  \"timestamp\": \"2024-09-16T12:00:00Z\",\n  \"hostname\": \"ip-172-16-0-12\"\n}\n</code></pre>"},{"location":"projeto/main/#estimativa-de-custos","title":"Estimativa de Custos","text":""},{"location":"projeto/main/#plano-atual-1-instancia","title":"Plano Atual (1 inst\u00e2ncia):","text":"Servi\u00e7o Plano Custo (USD/m\u00eas) Container Nano (512MB RAM, 0.25 vCPU) $7,00 PostgreSQL DB Gerenciado (1GB RAM, 2 vCPUs, 40GB) $15,00 Total mensal $22,00 <p>O custo total est\u00e1 abaixo do limite de $50/m\u00eas exigido pela rubrica.</p>"},{"location":"projeto/main/#projecoes-de-escalabilidade","title":"Proje\u00e7\u00f5es de Escalabilidade:","text":"N\u00ba de Inst\u00e2ncias Custo Estimado 1 $22,00 5 $110,00 10 $220,00"},{"location":"projeto/main/#midia-e-evidencias","title":"M\u00eddia e Evid\u00eancias","text":""},{"location":"projeto/main/#1-registro-de-usuario-registrar","title":"1. Registro de Usu\u00e1rio (<code>/registrar</code>)","text":""},{"location":"projeto/main/#2-login","title":"2. Login","text":"<p>Login bem-sucedido:</p> <p></p> <p>Tentativa de login inv\u00e1lido (usu\u00e1rio n\u00e3o cadastrado ou senha errada):</p> <p></p>"},{"location":"projeto/main/#3-consulta-consultar-com-token-jwt","title":"3. Consulta (<code>/consultar</code> com token JWT)","text":"<p>Token no Header:</p> <p></p> <p>Resposta com dados do scrap:</p> <p></p>"},{"location":"projeto/main/#4-health-check-health-check","title":"4. Health-check (<code>/health-check</code>)","text":"<ul> <li> Painel do AWS Lightsail com container ativo</li> <li> Banco de dados configurado</li> <li> Tela com custo atual da AWS</li> </ul>"},{"location":"projeto/main/#5-implantacao-na-aws","title":"5. Implanta\u00e7\u00e3o na AWS","text":""},{"location":"projeto/main/#a-variaveis-de-ambiente-do-container-e-imagem-utilizada","title":"a) Vari\u00e1veis de ambiente do container e imagem utilizada","text":""},{"location":"projeto/main/#b-plano-do-container-no-aws-lightsail","title":"b) Plano do container no AWS Lightsail","text":""},{"location":"projeto/main/#c-informacoes-da-instancia-de-banco-de-dados-gerenciado-postgresql","title":"c) Informa\u00e7\u00f5es da inst\u00e2ncia de banco de dados gerenciado (PostgreSQL)","text":""},{"location":"projeto/main/#d-detalhes-de-conexao-do-banco-usuario-endpoint-e-porta","title":"d) Detalhes de conex\u00e3o do banco (usu\u00e1rio, endpoint e porta)","text":""},{"location":"projeto/main/#e-api-publica-rodando-na-nuvem-interface-swagger","title":"e) API p\u00fablica rodando na nuvem (interface Swagger)","text":""},{"location":"projeto/main/#video-de-demonstracao","title":"V\u00eddeo de Demonstra\u00e7\u00e3o","text":"<p>\ud83d\udd17 Link para o v\u00eddeo: https://youtu.be/i6KeJAUAcUM</p>"},{"location":"projeto/main/#arquitetura-final-do-projeto","title":"Arquitetura Final do Projeto","text":"<p>A arquitetura do projeto foi desenhada de forma simples e eficiente, utilizando dois principais componentes implantados via AWS Lightsail:</p> <ol> <li>Container de aplica\u00e7\u00e3o</li> <li>Respons\u00e1vel por hospedar a API desenvolvida com FastAPI</li> <li>Publicado como imagem no Docker Hub (<code>marinnagc/api_cloud:latest</code>)</li> <li>Exp\u00f5e os endpoints para o cliente via dom\u00ednio p\u00fablico da AWS</li> <li> <p>Acessa o banco de dados atrav\u00e9s de uma <code>DATABASE_URL</code> segura</p> </li> <li> <p>Banco de dados gerenciado</p> </li> <li>Servi\u00e7o PostgreSQL fornecido pelo Lightsail</li> <li>Configurado com acesso p\u00fablico para a API</li> <li>Protegido por usu\u00e1rio e senha e restrito \u00e0 aplica\u00e7\u00e3o</li> </ol>"},{"location":"projeto/main/#componentes-e-comunicacao","title":"Componentes e Comunica\u00e7\u00e3o","text":"<ul> <li>App (FastAPI): roda dentro do container</li> <li>Container: gerenciado no Lightsail, com dom\u00ednio p\u00fablico</li> <li>Banco de dados PostgreSQL: inst\u00e2ncia separada, tamb\u00e9m no Lightsail</li> <li>Rede: comunica\u00e7\u00e3o direta da API com o banco via internet, com uso de vari\u00e1veis de ambiente</li> <li>Dom\u00ednio p\u00fablico: fornecido automaticamente pelo Lightsail, utilizado para acesso externo</li> </ul>"},{"location":"projeto/main/#diagrama-da-arquitetura","title":"Diagrama da Arquitetura","text":"<pre><code>flowchart TD\n    Usuario[Usu\u00e1rio] --&gt;|Requisi\u00e7\u00f5es HTTP| Container[Container Lightsail&lt;br&gt;FastAPI]\n    Container --&gt;|Conex\u00e3o TCP| Banco[(Banco de Dados&lt;br&gt;PostgreSQL gerenciado)]\n    Container --&gt;|Scraping| Web[Internet (3rd Party Data)]</code></pre>"},{"location":"roteiro1/main/","title":"Roteiro 1: MAAS","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<ul> <li> <p>Realizar uma experi\u00eancia pr\u00e1tica na configura\u00e7\u00e3o e gerenciamento de uma infraestrutura de nuvem bare-metal utilizando o MaaS (Metal as a Service)</p> </li> <li> <p>Aprender sobre conceitos b\u00e1sicos de rede de computadores </p> </li> </ul>"},{"location":"roteiro1/main/#infra","title":"Infra","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro1/main/#maas","title":"MAAS","text":""},{"location":"roteiro1/main/#instalando-o-maas","title":"Instalando o MAAS:","text":"sudo snap install maas --channel=3.5/Stable <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro1/main/#configurando-o-maas","title":"Configurando o MAAS","text":"<p>Primeiramente, foi inicializado o MAAS e criado o usu\u00e1rio para o todo o uso futuro, ap\u00f3s isso, foi-se criado um par de chaves para autentica\u00e7\u00e3o SSH.  Agora podemos configurar um DNS Forwarder com o DNS do Insper para que podemos usar este DNS para nosso roteador acessar. Importamos as imagens do Ubuntu para o uso futuro em nossas m\u00e1quinas, utilizamos das vers\u00f5es Ubuntu 22.04LTS e Ubuntu 20.04 LTS. Foi feito do upload da chave copiada anteriormente no terminal SSH do MAAS.</p>"},{"location":"roteiro1/main/#chaveando-o-dhcp","title":"Chaveando o DHCP","text":"<p>\u00c9 necess\u00e1rio habilitar o DHCP na subrede do MAAS, para que ele fa\u00e7a a distribui\u00e7\u00e3o dos Ip's nas nossas m\u00e1quinas, foi necess\u00e1rio alterar o Reserved Range para iniciar em 172.16.11.1 e acabar em 172.16.14.255 assim, garantimos que o MAAS n\u00e3o distribua Ip's em uma faixa que j\u00e1 esteja sendo usada pela rede. Tamb\u00e9m, foi necess\u00e1rio desabilitar o DHCP que o roteador estava nos fornecendo.</p>"},{"location":"roteiro1/main/#comissionamento","title":"Comissionamento","text":"<p>Com o MasAddress de cada m\u00e1quina, fizemos o comissionamento da m\u00e1quina main, roteador e outros 4 servers, esse comissionamento \u00e9 feito de form autom\u00e1tica realizando o boot via PXE na rede. Ap\u00f3s verificar se as espefifica\u00e7\u00f5es de m\u00e1quina estavam de acordo, seguimos com os pr\u00f3ximos passos.</p>"},{"location":"roteiro1/main/#acesso-remoto","title":"Acesso Remoto","text":"<p>O acesso remoto \u00e9 uma parte essencial para a realiza\u00e7\u00e3o do projeto, visto que, sem ele, seria necess\u00e1rio estar fisicamente presente no laborat\u00f3rio e conectar-se diretamente ao servidor por meio de um cabo de rede. Para possibilitar a conex\u00e3o externa a partir da rede Wi-Fi do Insper, foi configurado um Network Address Translation (NAT) no roteador do kit, permitindo o redirecionamento de requisi\u00e7\u00f5es externas para a porta 22 (SSH) do servidor principal, denominado MAIN.</p> <p>Dessa forma, com a configura\u00e7\u00e3o do NAT e a libera\u00e7\u00e3o do gerenciamento remoto, foi poss\u00edvel acessar o servidor MAIN externamente, por\u00e9m com a necessidade de estar conectado na rede do Insper, assim, garantindo maior flexibilidade no desenvolvimento e na execu\u00e7\u00e3o do projeto.</p>"},{"location":"roteiro1/main/#app","title":"App","text":""},{"location":"roteiro1/main/#django-em-nuvem","title":"Django em Nuvem","text":""},{"location":"roteiro1/main/#banco-de-dados-postgresql-no-server1","title":"Banco de Dados - PostgreSQL no Server1","text":"<p>Para que a aplica\u00e7\u00e3o Django funcionasse corretamente em nossa infraestrutura bare-metal, foi necess\u00e1rio primeiramente configurar o banco de dados PostgreSQL, que servir\u00e1 como backend compartilhado entre os servidores de aplica\u00e7\u00e3o.</p>"},{"location":"roteiro1/main/#instalacao-e-configuracao","title":"Instala\u00e7\u00e3o e configura\u00e7\u00e3o","text":"<p>Para essa tarefa, realizamos o deploy do sistema Ubuntu 22.04 no server1 utilizando o MAAS</p> <p>Ap\u00f3s o deploy, acessamos o servidor remotamente via SSH e seguimos com a instala\u00e7\u00e3o do PostgreSQL:</p> <pre><code>sudo apt update\nsudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Em seguida, foi criado um usu\u00e1rio e nosso banco de dados, ap\u00f3s isso, realizamos as configura\u00e7\u00f5es necess\u00e1rias e liberamos a porta utilizada pelo postgres.</p> <pre><code>sudo ufw allow 5432/tcp\n</code></pre>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<ol> <li>Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional</li> <li>Acessivel na pr\u00f3pria maquina na qual ele foi implantado.</li> <li>Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN.</li> <li>Em qual porta este servi\u00e7o est\u00e1 funcionando.</li> </ol> <p>Banco como Ativo</p> <p></p> <p>Acessivel na pr\u00f3pria maquina</p> <p></p> <p>Acessivel da MAIN</p> <p></p> <p>Porta que est\u00e1 rodando o servi\u00e7o</p>"},{"location":"roteiro1/main/#aplicacao-django","title":"Aplica\u00e7\u00e3o Django","text":"<p>Agora, precisamos subir a aplica\u00e7\u00e3o Django para nosso MAAS, para isso, vamos precisar \"pedir\" uma m\u00e1quina, e vamos fazer isso atravez do cli.</p> <p><pre><code>maas login [login] http://172.16.0.3:5240/MAAS/\n</code></pre> Em que [login] \u00e9 substituido pelo usu\u00e1rio do MAAS.</p> <p>Foi feito a reserva de nossa m\u00e1quina denominada \"server2\", logo, para acessar o servi\u00e7o do Django fora desta m\u00e1quina, foi necess\u00e1rio realizar um tunel SSH, que conectou a porta 8080 do server2 para a 8001 no localhost do computador.</p> <pre><code>ssh cloud@10.103.0.X -L 8001:[IP server2]:8080\n</code></pre>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<ol> <li>Do Dashboard do MAAS com as m\u00e1quinas.</li> <li>Da aba images, com as imagens sincronizadas.</li> <li>Da Aba de cada maquina(5x) mostrando os testes de hardware e commissioning com Status \"OK\"</li> </ol> <p>Dashboard do MAAS</p> <p></p> <p>Aba images</p> <p></p> <p>M\u00e1quina 1</p> <p></p> <p>M\u00e1quina 2</p> <p></p> <p>M\u00e1quina 3</p> <p></p> <p>M\u00e1quina 4</p> <p></p> <p>M\u00e1quina 5</p>"},{"location":"roteiro1/main/#uso-do-ansible","title":"Uso do Ansible","text":""},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<ol> <li>De um print da tela do Dashboard do MAAS com as 2 Maquinas e seus respectivos IPs.</li> <li>De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server</li> <li>Explique como foi feita a implementacao manual da aplicacao Django e banco de dados.</li> </ol> <p>Dashboard do MAAS com as 2 Maquinas e seus respectivos IPs</p> <p></p> <p>Aplica\u00e7\u00e3o Django</p> <p>A aplica\u00e7\u00e3o Django foi implementada manualmente em uma m\u00e1quina provisionada pelo MaaS. Ap\u00f3s instalar as depend\u00eancias (como Python, pip e PostgreSQL), foi configurado um ambiente virtual e instalado o Django com as bibliotecas necess\u00e1rias. Um banco de dados PostgreSQL foi criado e integrado \u00e0 aplica\u00e7\u00e3o por meio do arquivo de configura\u00e7\u00f5es. As migra\u00e7\u00f5es foram aplicadas e um superusu\u00e1rio foi criado. Por fim, a aplica\u00e7\u00e3o foi executada na porta 8080.</p>"},{"location":"roteiro1/main/#utilizando-ansible","title":"Utilizando Ansible","text":"<p>Agora passamos a utilizar duas inst\u00e2ncias da aplica\u00e7\u00e3o Django (server2 e server3) conectadas ao mesmo banco de dados (em server1). Isso permite alta disponibilidade e balanceamento de carga. Para facilitar a instala\u00e7\u00e3o automatizada em m\u00faltiplos servidores, foi utilizado o Ansible, que permite gerenciar v\u00e1rias m\u00e1quinas de forma idempotente e eficiente. Com um playbook, foi poss\u00edvel configurar rapidamente o server3 com a mesma aplica\u00e7\u00e3o j\u00e1 implantada no server2.</p>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<ol> <li>De um print da tela do Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs.</li> <li>De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server2 </li> <li>De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server3 </li> <li>Explique qual diferenca entre instalar manualmente a aplicacao Django e utilizando o Ansible.</li> </ol> <p>Dashboard MAAS</p> <p></p> <p>Aplica\u00e7\u00e3o Django conectado no server2</p> <p></p> <p>Aplica\u00e7\u00e3o Django conectado no server3</p> <p>A diferen\u00e7a est\u00e1 principalmente na automa\u00e7\u00e3o e consist\u00eancia:</p> <p>Instala\u00e7\u00e3o manual: Voc\u00ea executa os passos (instala pacotes, configura arquivos etc.) manualmente em cada servidor. Se precisar instalar em v\u00e1rios servidores, precisa repetir todos os passos um a um, sujeito a erros humanos e inconsist\u00eancias.</p> <p>Instala\u00e7\u00e3o via Ansible: Voc\u00ea escreve um playbook que descreve de forma declarativa tudo o que precisa para instalar e configurar a aplica\u00e7\u00e3o (pacotes, arquivos, permiss\u00f5es etc.). O Ansible ent\u00e3o roda esse playbook em m\u00faltiplos servidores simultaneamente, garantindo que todos fiquem exatamente no mesmo estado. Ele \u00e9 idempotente, ou seja, se voc\u00ea rodar o playbook v\u00e1rias vezes, n\u00e3o vai \u201cquebrar\u201d ou refazer desnecessariamente o que j\u00e1 est\u00e1 pronto. Isso facilita muito quando se precisa gerenciar diversos servidores.</p>"},{"location":"roteiro1/main/#balancamento-de-carga-usando-proxy-reverso","title":"Balancamento de carga usando Proxy Reverso","text":"<p>O balanceamento de carga com proxy reverso \u00e9 uma t\u00e9cnica que melhora a disponibilidade e o desempenho de aplica\u00e7\u00f5es web. Em vez dos usu\u00e1rios acessarem diretamente os servidores que rodam a aplica\u00e7\u00e3o Django (como server2 e server3), eles se conectam a um \u00fanico ponto de entrada \u2014 o proxy reverso, instalado no server4.</p> <p>Esse proxy (geralmente usando o NGINX) recebe as requisi\u00e7\u00f5es dos usu\u00e1rios e as distribui entre os servidores dispon\u00edveis, conforme a carga. Isso traz duas grandes vantagens:</p> <ul> <li> <p>Alta disponibilidade: se um dos servidores cair, o outro continua atendendo.</p> </li> <li> <p>Desempenho otimizado: o tr\u00e1fego \u00e9 dividido, evitando sobrecarga em um \u00fanico servidor.</p> </li> </ul> <p>Essa arquitetura centraliza o acesso e torna o sistema mais robusto, escal\u00e1vel e resiliente a falhas.</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<ol> <li>De um print da tela do Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs.</li> <li>Altere o conte\u00fado da mensagem contida na fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> de cada server para distinguir ambos os servers. </li> <li>Fa\u00e7a um <code>GET request</code> para o path que voce criou em urls.py para o Nginx e tire 2 prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.</li> </ol> <p>Dashboard MAAS</p> <p></p> <p>Mensagem alterada acessada pelo server2</p> <p></p> <p>Mensagem alterada acessada pelo server4</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Durante a execu\u00e7\u00e3o do roteiro, algumas dificuldades t\u00e9cnicas se destacaram. Uma delas foi o entendimento inicial da configura\u00e7\u00e3o de redes, principalmente no que diz respeito ao papel do DHCP e da sub-rede gerenciada pelo MAAS. A necessidade de desativar o DHCP do roteador e ajustar corretamente o intervalo reservado para IPs exigiu aten\u00e7\u00e3o especial para evitar conflitos de rede.</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>A realiza\u00e7\u00e3o deste roteiro permitiu aplicar na pr\u00e1tica diversos conceitos essenciais para o gerenciamento de uma infraestrutura em nuvem bare-metal, como o provisionamento automatizado de servidores, o gerenciamento de redes, o acesso remoto seguro, e a implanta\u00e7\u00e3o de aplica\u00e7\u00f5es distribu\u00eddas.</p> <p>O uso do MAAS mostrou-se eficaz para orquestrar os recursos f\u00edsicos dispon\u00edveis, enquanto ferramentas como Ansible e NGINX agregaram valor em automa\u00e7\u00e3o e escalabilidade. Foi poss\u00edvel comprovar a import\u00e2ncia de uma arquitetura distribu\u00edda bem planejada, que permita a escalabilidade e mantenha a resili\u00eancia do sistema mesmo em caso de falhas individuais.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2: JUJU","text":""},{"location":"roteiro2/main/#1-objetivo","title":"1. Objetivo","text":"<p>O principal objetivo desta atividade \u00e9 compreender e aplicar os conceitos de Deployment Orchestration, utilizando a ferramenta Juju para realizar o gerenciamento e a orquestra\u00e7\u00e3o de aplica\u00e7\u00f5es distribu\u00eddas sobre uma infraestrutura bare metal, previamente configurada com MAAS. Tamb\u00e9m \u00e9 objetivo entender como as aplica\u00e7\u00f5es se comunicam entre si, realizando o deploy e a integra\u00e7\u00e3o de duas ferramentas populares de monitoramento: Prometheus e Grafana.</p>"},{"location":"roteiro2/main/#2-infraestrutura","title":"2. Infraestrutura","text":"<p>A infraestrutura foi baseada em uma integra\u00e7\u00e3o entre MAAS e Juju. Constru\u00edda com o foco em permitir que o Juju utilizasse o MAAS como seu provedor de recursos f\u00edsicos e de sistema operacional.</p>"},{"location":"roteiro2/main/#21-adicionando-o-cluster-maas-como-uma-cloud-para-o-juju","title":"2.1 Adicionando o Cluster MAAS como uma Cloud para o Juju","text":"<p>A primeira etapa foi garantir que o Juju reconhecesse o MAAS como um cloud provider (provedor de cloud), ou seja, como uma fonte de m\u00e1quinas f\u00edsicas e capacidade de provisionamento. Para isso, criamos um arquivo chamado <code>maas-cloud.yaml</code>, com a seguinte estrutura:</p> <p><pre><code>clouds:\n  maas-one:\n    type: maas\n    auth-types: [oauth1]\n    endpoint: http://192.168.0.3:5240/MAAS/\n</code></pre> Esse arquivo informa ao Juju onde encontrar o servidor MAAS, qual tipo de autentica\u00e7\u00e3o usar e que tipo de provedor ele est\u00e1 adicionando. Com ele, o comando abaixo adicionou o MAAS como uma cloud no Juju:</p> <pre><code>juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>Agora o Juju passou a enxergar o MAAS como uma \"nuvem privada\", podendo requisitar m\u00e1quinas quando necess\u00e1rio.</p>"},{"location":"roteiro2/main/#22-adicionando-as-credenciais-do-maas-ao-juju","title":"2.2 Adicionando as credenciais do MAAS ao Juju","text":"<p>Como toda integra\u00e7\u00e3o precisa de autoriza\u00e7\u00e3o, a pr\u00f3xima etapa foi criar o arquivo <code>maas-creds.yaml</code>,, contendo as credenciais necess\u00e1rias para autentica\u00e7\u00e3o via API do MAAS , com a chave de autentica\u00e7\u00e3o OAuth1 gerada pelo MAAS:</p> <pre><code>credentials:\n  maas-one:\n    anyuser:\n      auth-type: oauth1\n      maas-oauth: &lt;API KEY&gt;\n</code></pre> <p>Essa chave \u00e9 \u00fanica para cada usu\u00e1rio MAAS e pode ser copiada no menu do usu\u00e1rio (API Key). Com esse arquivo pronto, rodamos o comando:</p> <pre><code>juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre> <p>Agora o Juju estava apto a interagir com a \"cloud\" <code>maas-one</code>, com permiss\u00f5es reais para alocar, instalar, apagar e monitorar m\u00e1quinas.</p>"},{"location":"roteiro2/main/#23-criacao-do-controlador","title":"2.3 Cria\u00e7\u00e3o do Controlador","text":"<p>Para que o Juju pudesse orquestrar os deploys, foi necess\u00e1rio criar um controlador.</p> <p>O controlador do Juju \u00e9 como o \"c\u00e9rebro\" da opera\u00e7\u00e3o: ele ser\u00e1 respons\u00e1vel por monitorar o estado do cluster, das aplica\u00e7\u00f5es, dos modelos, das m\u00e1quinas e dos servi\u00e7os. Para criar o controlador:</p> <ol> <li>Acessamos o Dashboard do MAAS.</li> <li>Escolhemos a m\u00e1quina <code>server1</code> e adicionamos a tag <code>juju</code>.</li> <li>Executamos o comando:</li> </ol> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Esse comando faz com que o Juju reserve a m\u00e1quina com a tag <code>juju</code>, instale o sistema operacional Ubuntu s\u00e9rie <code>jammy</code>, e configure toda a infraestrutura b\u00e1sica do controlador.</p> <p>Esse processo leva alguns minutos pois envolve: - Aloca\u00e7\u00e3o do hardware f\u00edsico. - Provisionamento autom\u00e1tico de sistema operacional. - Instala\u00e7\u00e3o dos componentes internos do Juju. - Sincroniza\u00e7\u00e3o entre Juju e MAAS.</p>"},{"location":"roteiro2/main/#24-papel-do-controlador-juju","title":"2.4 Papel do Controlador Juju","text":"<p>O Juju Controller \u00e9 uma m\u00e1quina exclusiva alocada para gerenciar o ciclo de vida das aplica\u00e7\u00f5es, armazenar informa\u00e7\u00f5es de estado, e executar comandos relacionados \u00e0 orquestra\u00e7\u00e3o. Todo o restante da configura\u00e7\u00e3o e deploy \u00e9 feito a partir da CLI <code>juju</code>, que agora substitui o <code>maas-cli</code> nas intera\u00e7\u00f5es com os servi\u00e7os em nuvem criados.</p>"},{"location":"roteiro2/main/#3-aplicacao","title":"3. Aplica\u00e7\u00e3o","text":"<p>Com a infraestrutura em funcionamento, utilizamos o Juju para orquestrar o deploy de duas aplica\u00e7\u00f5es populares em ambientes de monitoramento: Prometheus e Grafana.</p> <ul> <li>Prometheus \u00e9 um sistema de monitoramento e banco de dados de s\u00e9ries temporais, ideal para coletar m\u00e9tricas de servidores e aplica\u00e7\u00f5es.</li> <li>Grafana \u00e9 uma plataforma de visualiza\u00e7\u00e3o que consome dados de diversas fontes (inclusive Prometheus) e apresenta gr\u00e1ficos e pain\u00e9is interativos para facilitar o entendimento e o diagn\u00f3stico do sistema.</li> </ul>"},{"location":"roteiro2/main/#31-instalacao-do-dashboard-do-juju","title":"3.1 Instala\u00e7\u00e3o do Dashboard do Juju","text":"<p>Instalamos o Dashboard do Juju, que \u00e9 uma interface gr\u00e1fica acess\u00edvel via navegador, que exibe: - As clouds configuradas. - Os modelos de aplica\u00e7\u00f5es. - O estado das m\u00e1quinas e servi\u00e7os. - Os logs e m\u00e9tricas de cada unidade.</p> <p>Esse dashboard \u00e9 essencial para valida\u00e7\u00e3o visual. Al\u00e9m disso, permite gerenciamento simplificado para quem n\u00e3o quer usar apenas a linha de comando.</p>"},{"location":"roteiro2/main/#32-deploy-da-aplicacao-grafana-e-prometheus","title":"3.2 Deploy da aplica\u00e7\u00e3o Grafana e Prometheus","text":"<p>Para preparar o ambiemte, criamos uma pasta para armazenar os charms (pacotes que o Juju usa para instalar e configurar aplica\u00e7\u00f5es), e em seguida fizemos o download dos mesmos.</p> <pre><code>mkdir -p /home/cloud/charms\ncd /home/cloud/charms\n</code></pre> <pre><code>juju download grafana\njuju download prometheus2\n</code></pre> <p>Pacotes do CharmHub, contendo scripts e metadados necess\u00e1rios para instala\u00e7\u00e3o das aplica\u00e7\u00f5es.</p>"},{"location":"roteiro2/main/#33-deploy-com-o-juju","title":"3.3 Deploy com o Juju","text":"<p>Realizamos o deploy local dos charms baixados:</p> <pre><code>juju deploy ./prometheus2_*.charm\njuju deploy ./grafana_*.charm\n</code></pre> <p>Durante o deploy, utilizamos o comando:</p> <pre><code>watch -n 1 juju status\n</code></pre> <p>Esse comando atualiza em tempo real o status de cada unidade implantada, facilitando o acompanhamento do progresso e a identifica\u00e7\u00e3o de poss\u00edveis erros.</p>"},{"location":"roteiro2/main/#34-integracao-do-grafana-com-prometheus","title":"3.4 Integra\u00e7\u00e3o do Grafana com Prometheus","text":"<p>Uma vez que ambas as aplica\u00e7\u00f5es estavam ativas, fizemos a integra\u00e7\u00e3o com base na documenta\u00e7\u00e3o oficial dos charms:</p> <ol> <li>Acessamos o dashboard do Grafana pelo navegador, usando o IP da m\u00e1quina onde ele foi instalado.</li> <li>Adicionamos o Prometheus como fonte de dados, informando o endpoint padr\u00e3o do servi\u00e7o.</li> <li>Criamos um painel de exemplo, com um gr\u00e1fico que consome dados do Prometheus (por exemplo, uso de CPU ou mem\u00f3ria).</li> </ol> <p>Essa etapa confirmou que a comunica\u00e7\u00e3o entre os servi\u00e7os estava correta e que o Grafana era capaz de interpretar as m\u00e9tricas fornecidas.</p> <p>Tarefa-1</p> <ol> <li>\ud83d\udcf8 D\u00ea um print da tela do Dashboard do MAAS com as m\u00e1quinas e seus respectivos IPs.  </li> <li>\ud83d\udcf8 D\u00ea um print de tela do comando <code>juju status</code> depois que o Grafana estiver com status <code>\"active\"</code>.  </li> <li>\ud83d\udcf8 D\u00ea um print da tela do Dashboard do Grafana com o Prometheus aparecendo como fonte de dados (source).  </li> <li>\ud83c\udf10 Prove (com print) que voc\u00ea est\u00e1 conseguindo acessar o Dashboard a partir da rede do Insper.  </li> <li>\ud83e\udde9 D\u00ea um print na tela que mostra as aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU    (ex: http://IP-Servi\u00e7o:8080/models/admin/maas)</li> </ol>"},{"location":"roteiro2/main/#tarefa-11","title":"Tarefa 1.1","text":""},{"location":"roteiro2/main/#tarefa-12","title":"Tarefa 1.2","text":""},{"location":"roteiro2/main/#tarefa-13","title":"Tarefa 1.3","text":""},{"location":"roteiro2/main/#tarefa-14","title":"Tarefa 1.4","text":""},{"location":"roteiro2/main/#tarefa-15","title":"Tarefa 1.5","text":""},{"location":"roteiro3/main/","title":"Roteiro 3: Openstack","text":""},{"location":"roteiro3/main/#explicacao-dos-deploys","title":"Explica\u00e7\u00e3o dos deploys","text":""},{"location":"roteiro3/main/#1-instalando-o-juju-controller","title":"1. Instalando o Juju Controller","text":"<p>Prepara uma m\u00e1quina dedicada (server1) para ser o controlador Juju, que orquestrar\u00e1 todos os deployments de charms no ambiente MAAS. </p>"},{"location":"roteiro3/main/#2-definindo-o-modelo-de-deploy","title":"2. Definindo o modelo de deploy","text":"<p>Cria e seleciona um modelo chamado openstack no Juju, definindo a s\u00e9rie padr\u00e3o (jammy) para todo o deploy. Isso isola o ambiente OpenStack em um espa\u00e7o de trabalho pr\u00f3prio. </p>"},{"location":"roteiro3/main/#3-ceph-osd","title":"3. Ceph OSD","text":"<p>Implanta tr\u00eas unidades do charm ceph-osd nos n\u00f3s de compute, configurando-os para usar /dev/sda e /dev/sdb como dispositivos de armazenamento. Esses OSDs (Object Storage Daemons) ser\u00e3o respons\u00e1veis por armazenar os dados em blocos do cluster Ceph. </p>"},{"location":"roteiro3/main/#4-nova-compute","title":"4. Nova Compute","text":"<p>Instala o charm nova-compute em cada n\u00f3 de compute (m\u00e1quinas 0, 1 e 2), transformando-os em hipervisores QEMU capazes de executar inst\u00e2ncias de m\u00e1quinas virtuais. Habilita live-migration e resize para mobilidade e escalabilidade das VMs. </p>"},{"location":"roteiro3/main/#5-mysql-innodb-cluster","title":"5. MySQL InnoDB Cluster","text":"<p>Desdobra tr\u00eas r\u00e9plicas do charm mysql-innodb-cluster em containers LXD, criando um cluster de banco de dados altamente dispon\u00edvel que servir\u00e1 como backend de dados para todos os servi\u00e7os OpenStack. </p>"},{"location":"roteiro3/main/#6-vault","title":"6. Vault","text":"<p>Utiliza o charm vault num container LXD (machine 2) para gerenciar certificados TLS e segredos. Em seguida, usa o charm mysql-router para relacionar o Vault ao cluster MySQL, garantindo comunica\u00e7\u00e3o segura entre servi\u00e7os. </p>"},{"location":"roteiro3/main/#7-neutron-networking","title":"7. Neutron Networking","text":"<p>Implanta quatro charms \u2014 neutron-api, neutron-api-plugin-ovn (subordinate), ovn-central e ovn-chassis \u2014 para prover a camada de rede virtual (flat e overlay) \u00e0s VMs. Configura mappings entre pontes OVS e interfaces f\u00edsicas e adiciona todas as rela\u00e7\u00f5es necess\u00e1rias, inclusive com o Vault e o banco de dados. </p>"},{"location":"roteiro3/main/#8-keystone","title":"8. Keystone","text":"<p>Instala o charm keystone (servi\u00e7o de identidade) e o relaciona ao cluster MySQL e ao Neutron para autentica\u00e7\u00e3o unificada dos usu\u00e1rios e servi\u00e7os. </p>"},{"location":"roteiro3/main/#9-rabbitmq","title":"9. RabbitMQ","text":"<p>Utiliza o rabbitmq-server como broker AMQP, adicionando rela\u00e7\u00f5es com o Neutron e o Nova Compute para fila de mensagens entre componentes distribu\u00eddos. </p>"},{"location":"roteiro3/main/#10-nova-cloud-controller","title":"10. Nova Cloud Controller","text":"<p>Instala o nova-cloud-controller (inclui scheduler, API e conductor) na m\u00e1quina 2, juntando-o ao banco de dados, ao Keystone, ao RabbitMQ, ao Neutron e ao Nova Compute para orquestrar pedidos de cria\u00e7\u00e3o e gerenciamento de VMs. </p>"},{"location":"roteiro3/main/#11-placement","title":"11. Placement","text":"<p>Desdobra o placement API na m\u00e1quina 2 e relaciona-o ao MySQL e ao Nova Cloud Controller, permitindo o rastreamento da disponibilidade de recursos (CPU, RAM, disco) para aloca\u00e7\u00e3o de inst\u00e2ncias. </p>"},{"location":"roteiro3/main/#12-horizon-openstack-dashboard","title":"12. Horizon \u2013 OpenStack Dashboard","text":"<p>Instala o openstack-dashboard (Horizon) para fornecer uma interface web de gerenciamento. \u00c9 conectado ao Keystone, ao MySQL e ao Vault para autentica\u00e7\u00e3o, armazenamento e TLS. </p>"},{"location":"roteiro3/main/#13-glance","title":"13. Glance","text":"<p>Implanta o glance (servi\u00e7o de imagens) na m\u00e1quina 2, relacionando-o ao banco de dados, ao Nova Compute e ao Keystone, para armazenar e servir imagens de disco das VMs. </p>"},{"location":"roteiro3/main/#14-ceph-monitor","title":"14. Ceph Monitor","text":"<p>Desdobra tr\u00eas unidades do ceph-mon para manter o mapa de cluster Ceph e garantir consist\u00eancia. Relaciona-se aos OSDs, ao Nova Compute e ao Glance para fornecer backend de armazenamento via RBD. </p>"},{"location":"roteiro3/main/#15-cinder","title":"15. Cinder","text":"<p>Instala o cinder (block storage service) na m\u00e1quina 1 e o cinder-ceph (subordinate) para usar o Ceph como backend de volumes persistentes. Relaciona-se tamb\u00e9m ao Nova Cloud Controller, ao Keystone, ao RabbitMQ e ao Glance. </p>"},{"location":"roteiro3/main/#16-ceph-rados-gateway","title":"16. Ceph RADOS Gateway","text":"<p>Utiliza o ceph-radosgw na m\u00e1quina 0, criando um gateway HTTP compat\u00edvel com S3/Swift para oferecer armazenamento de objetos separado do Swift nativo. Relaciona-se ao cluster de Monitores Ceph. </p>"},{"location":"roteiro3/main/#17-ceph-osd-integration","title":"17. Ceph-OSD Integration","text":"<p>Ajusta a configura\u00e7\u00e3o de dispositivos do ceph-osd para /dev/sdb, integrando por completo os OSDs ap\u00f3s garantir que todos os passos anteriores foram conclu\u00eddos sem erros. </p>"},{"location":"roteiro3/main/#como-os-componentes-se-conectam","title":"Como os componentes se conectam","text":"<p>Todos os servi\u00e7os s\u00e3o integrados via rela\u00e7\u00f5es Juju, que automaticamente criam e gerenciam canais de comunica\u00e7\u00e3o entre charms:</p>"},{"location":"roteiro3/main/#banco-de-dados-mysql","title":"Banco de dados (MySQL)","text":"<p>Cada servi\u00e7o que precisa de persist\u00eancia (Keystone, Nova, Neutron, Glance, Cinder, Horizon, Placement, Vault) \u00e9 relacionado ao cluster MySQL via mysql-router;</p>"},{"location":"roteiro3/main/#secret-management-vault","title":"Secret Management (Vault)","text":"<p>Fornece certificados TLS a todos os servi\u00e7os que exigem comunica\u00e7\u00e3o segura, via vault:certificates;</p>"},{"location":"roteiro3/main/#mensageria-rabbitmq","title":"Mensageria (RabbitMQ)","text":"<p>Broker AMQP ligado a Neutron e Nova Compute para filas de trabalho e eventos;</p>"},{"location":"roteiro3/main/#identidade-keystone","title":"Identidade (Keystone)","text":"<p>Autenticador \u00fanico, integrado a Neutron, Nova Cloud Controller e Horizon via identity-service;</p>"},{"location":"roteiro3/main/#armazenamento-ceph","title":"Armazenamento (Ceph)","text":"<p>OSDs e Monitores formam o backend distribu\u00eddo; Nova Compute, Glance e Cinder consomem esse armazenamento via rela\u00e7\u00f5es ceph:client, ceph-access e storage-backend;</p>"},{"location":"roteiro3/main/#rede-neutronovn","title":"Rede (Neutron/OVN)","text":"<p>Neutron API, Central, Chassis e plugin OVN estruturam as redes flat e overlay, relacionando-se ao Nova Compute para vincular interfaces de VMs;</p>"},{"location":"roteiro3/main/#dashboard-horizon","title":"Dashboard (Horizon)","text":"<p>Consome Keystone para login e MySQL para dados administrativos.</p>"},{"location":"roteiro3/main/#setup","title":"SetUp","text":""},{"location":"roteiro3/main/#1-autenticacao-com-keystone","title":"1. Autentica\u00e7\u00e3o com Keystone","text":"<ul> <li>Carregamos o arquivo de credenciais (openrc) em nosso terminal de administra\u00e7\u00e3o, para assegurar que todos os sentimentos dos servi\u00e7os (Nova, Cinder, Neutron) seriam invocados com privil\u00e9gios corretos e para garantir seguran\u00e7a e rastreabilidade das a\u00e7\u00f5es.</li> </ul>"},{"location":"roteiro3/main/#tarefa-11","title":"Tarefa 1.1","text":""},{"location":"roteiro3/main/#tarefa-12","title":"Tarefa 1.2","text":""},{"location":"roteiro3/main/#tarefa-13","title":"Tarefa 1.3","text":""},{"location":"roteiro3/main/#tarefa-14","title":"Tarefa 1.4","text":""},{"location":"roteiro3/main/#tarefa-15","title":"Tarefa 1.5","text":""},{"location":"roteiro3/main/#2-verificacao-inicial-na-interface-horizon","title":"2. Verifica\u00e7\u00e3o Inicial na Interface Horizon","text":"<ul> <li>Acessamos o painel Horizon como administradores e navegamos pelas se\u00e7\u00f5es principais: Compute, Inst\u00e2ncias e Network Topology, para obter uma vis\u00e3o geral do estado atual do ambiente \u2014 m\u00e1quinas provisionadas pelo MaaS e unidades do Juju.</li> </ul>"},{"location":"roteiro3/main/#3-preparacao-de-imagens-e-flavors","title":"3. Prepara\u00e7\u00e3o de Imagens e Flavors","text":"<ul> <li>Importamos a imagem base do Ubuntu Jammy e criamos quatro perfis de inst\u00e2ncia (flavors) com diferentes combina\u00e7\u00f5es de CPU, mem\u00f3ria e disco, isso nos permitiu padronizar os tipos de VMs que seriam instanciadas posteriormente.</li> </ul>"},{"location":"roteiro3/main/#4-configuracao-da-rede-externa","title":"4. Configura\u00e7\u00e3o da Rede Externa","text":"<ul> <li>Definimos uma rede externa e alocamos um pool de endere\u00e7os na faixa pr\u00e9-estabelecida (172.16.7.0\u2013172.16.8.255), para conectar as VMs \u00e0 rede f\u00edsica e possibilitar comunica\u00e7\u00e3o com o mundo externo, mantendo o controle de escopo do tr\u00e1fego de sa\u00edda.</li> </ul>"},{"location":"roteiro3/main/#5-montagem-da-rede-interna-e-roteador-virtual","title":"5. Montagem da Rede Interna e Roteador Virtual","text":"<ul> <li>Criamos uma rede privada isolada (192.169.0.0/24) e associamos um roteador virtual que faz o link entre essa rede interna e a externa, para segmentar o tr\u00e1fego interno das inst\u00e2ncias, garantindo isolamento e flexibilidade de roteamento.</li> </ul>"},{"location":"roteiro3/main/#6-importacao-de-chave-ssh-e-regras-de-seguranca","title":"6. Importa\u00e7\u00e3o de Chave SSH e Regras de Seguran\u00e7a","text":"<ul> <li>Carregamos nosso par de chaves SSH (id_rsa.pub) para permitir acessos sem senha e ajustamos o grupo de seguran\u00e7a padr\u00e3o para autorizar SSH e ICMP. Como o nome do nosso par de chave \u00e9 padr\u00e3o utilizamos o comando:      ssh ubuntu@172.16.8.31  Sendo o IP utilizado o nosso Floating IP.</li> </ul>"},{"location":"roteiro3/main/#7-teste-de-criacao-de-instancia","title":"7. Teste de Cria\u00e7\u00e3o de Inst\u00e2ncia","text":"<ul> <li>Subimos uma VM do tipo m1.tiny chamada <code>client</code>, associamos um IP flutuante e validamos acesso SSH desde nossa m\u00e1quina local, para confirmar que todas as configura\u00e7\u00f5es anteriores (imagens, flavors, redes, seguran\u00e7a) estavam corretas e funcionais.</li> </ul>"},{"location":"roteiro3/main/#tarefa-11_1","title":"Tarefa 1.1","text":""},{"location":"roteiro3/main/#tarefa-12_1","title":"Tarefa 1.2","text":""},{"location":"roteiro3/main/#tarefa-13_1","title":"Tarefa 1.3","text":""},{"location":"roteiro3/main/#tarefa-14_1","title":"Tarefa 1.4","text":""},{"location":"roteiro3/main/#tarefa-21","title":"Tarefa 2.1","text":"<ul> <li> <p>Inst\u00e2ncias: inexistentes na Tarefa 1 \u00d7 inst\u00e2ncia \u201cclient\u201d rodando na Tarefa 2.</p> </li> <li> <p>Limite de recursos (Overview): todos zerados na Tarefa 1 \u00d7 uso de inst\u00e2ncia, rede, porta e roteador em Tarefa 2.</p> </li> <li> <p>Topologia de rede: nenhuma rede ou roteador na Tarefa 1 \u00d7 redes (ext_net, user1_net), roteador (user1_router) e inst\u00e2ncia conectada em Tarefa 2.</p> </li> </ul>"},{"location":"roteiro3/main/#tarefa-31","title":"Tarefa 3.1","text":"<p>Explicado em cada etapa mostrada acima.</p>"},{"location":"roteiro3/main/#8-escalabilidade-dos-nos-de-computacao-e-armazenamento","title":"8. Escalabilidade dos N\u00f3s de Computa\u00e7\u00e3o e Armazenamento","text":"<ul> <li>Adicionamos unidades de compute e de block storage ao cluster via Juju (nova-compute e ceph-osd) em m\u00e1quinas reservadas no MaaS, para demonstrar a escalabilidade horizontal do ambiente, aumentando capacidade de processamento e balanceamento de carga.</li> </ul>"},{"location":"roteiro3/main/#exemplo-de-diagrama-mermaid","title":"Exemplo de Diagrama (Mermaid)","text":"<pre><code>flowchart TB\n  subgraph Campus[\"Campus Insper\"]\n    Internet[\"Internet / Campus Network\"]\n    MaaS[\"MaaS Controller\"]\n    Juju[\"Juju Controller\"]\n  end\n\n  subgraph OpenStack[\"OpenStack\"]\n    Ext[\"Rede Ext. (172.16.7.0/24)\"]\n    Rtr[\"Virtual Router\"]\n    Int[\"Rede Int. (192.169.0.0/24)\"]\n    VM[\"VM: client (m1.tiny)\nPriv: 192.169.0.5\"]\n  end\n\n  Internet --&gt; Ext\n  Ext --&gt; Rtr\n  Rtr --&gt; Int\n  Int --&gt; VM\n  VM --&gt;|Floating: 172.16.8.31| Ext</code></pre>"},{"location":"roteiro3/main/#app","title":"App","text":"<p>Nesta etapa alocamos uma API construida pelo FastApi em nosso projeto, foi dividido desta maneira: </p> <ul> <li>2 inst\u00e2ncias com a API do projeto, etapa 1</li> <li>1 inst\u00e2ncia com banco de dados, etapa 1, e</li> <li>1 inst\u00e2ncia com LoadBalancer, Nginx.</li> </ul>"},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":"<p>A seguir temos nossa visualiza\u00e7\u00e3o da API criada, al\u00e9m das abas network topology, demosntrando a estrutura de rede, e cada uma de nossas m\u00e1quinas, com seus IP's flutuantes e flavors</p>"},{"location":"roteiro3/main/#dashboard-api","title":"Dashboard API","text":""},{"location":"roteiro3/main/#network-topology","title":"Network Topology","text":""},{"location":"roteiro3/main/#app-1","title":"App 1","text":""},{"location":"roteiro3/main/#app-2","title":"App 2","text":""},{"location":"roteiro3/main/#nginx","title":"Nginx","text":""},{"location":"roteiro3/main/#banco-de-dados","title":"Banco de dados","text":""},{"location":"roteiro4/main/","title":"IaC","text":"<p>Infraestrutura como C\u00f3digo (IaC) \u00e9 uma pr\u00e1tica que traz os princ\u00edpios de desenvolvimento de software para o gerenciamento e provisionamento de infraestrutura de TI. Em vez de configurar manualmente servidores, redes e demais componentes, tudo \u00e9 descrito em arquivos de configura\u00e7\u00e3o versionados, permitindo que ambientes sejam criados, modificados e replicados de forma autom\u00e1tica e previs\u00edvel.</p>"},{"location":"roteiro4/main/#terraform-como-iac","title":"Terraform como Iac","text":"<p>Terraform \u00e9 uma ferramenta de Infraestrutura como C\u00f3digo (IaC) desenvolvida pela HashiCorp que permite definir, versionar e gerenciar recursos de infraestrutura em m\u00faltiplas nuvens e provedores por meio de c\u00f3digo declarativo. A seguir, os principais aspectos de como o Terraform se encaixa no paradigma de IaC:</p> <ul> <li> <p>Arquivos de configura\u00e7\u00e3o em HCL Linguagem HCL (HashiCorp Configuration Language): sintaxe leg\u00edvel por humanos, com blocos como provider, resource, variable e output.</p> </li> <li> <p>Estado e Idempot\u00eancia Estado remoto: o Terraform mant\u00e9m um arquivo de estado (terraform.tfstate) que registra o que foi provisionado. Com isso, ele sabe exatamente o que criar, alterar ou destruir.</p> </li> </ul> <p>Idempot\u00eancia: ao rodar terraform apply m\u00faltiplas vezes sobre o mesmo conjunto de configura\u00e7\u00f5es, o Terraform s\u00f3 aplicar\u00e1 mudan\u00e7as que alterem o estado atual para o desejado \u2014 sem recriar recursos desnecessariamente.</p>"},{"location":"roteiro4/main/#instalar-o-terraform","title":"Instalar o terraform","text":"<p>Para instalar o terraform, foi necess\u00e1rio os seguintes comandos:  <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\n</code></pre> <pre><code>gpg --no-default-keyring --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg --fingerprint\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n</code></pre> <pre><code>sudo apt update &amp;&amp; sudo apt install terraform\n</code></pre></p>"},{"location":"roteiro4/main/#infra","title":"Infra","text":"<p>Neste trabalho, foi configurada a hierarquia de projetos no OpenStack via Horizon para isolar os recursos de cada aluno. Primeiramente, criou-se um dom\u00ednio denominado AlunosDomain. Em seguida, gerou-se para cada estudante um projeto seguindo o padr\u00e3o Kit_ e definiram-se cotas espec\u00edficas conforme a necessidade. Ap\u00f3s isso, criaram-se os usu\u00e1rios ( \u201cJo\u00e3o\u201d e \u201cMarinna\u201d), associaram-se cada um ao seu respectivo projeto em AlunosDomain e atribu\u00edram-se a eles o papel de administrador. Por fim, validaram-se as permiss\u00f5es navegando em Identity &gt; Projects, acessando a aba Members de cada projeto e confirmando que os usu\u00e1rios dispunham dos pap\u00e9is corretos para gerenciar seus recursos."},{"location":"roteiro4/main/#app","title":"App","text":""},{"location":"roteiro4/main/#criando-a-infraestrutura-utilizando-iac","title":"Criando a Infraestrutura utilizando IaC","text":"<p>O Terraform conta com arquivos .tf, que definem a infraestrutura, o Terraform sabe o que \u00e9 implantado por meio do arquivo de estado. Este estado \u00e9 armazenado por padr\u00e3o em um arquivo local chamado \u201cterraform.tfstate\u201d.</p> <p>O Terraform permite v\u00e1rios provedores, normalmente inicializados em um arquivo chamado provider.tf.</p> <p>A estrutura de pastas ficar\u00e1 conforme abaixo:    --Terraform                                                    |--instance.tf                                                    |--network.tf                                                    |--provider.tf                                                    |--router.tf</p> <p>Isso foi feito para cada aluno, em uma pasta para cada.</p> <p>Foi criado os arquivos abaixo:</p> <p>provider.tf <pre><code># Terraform Openstack deployment\n# Author: Tiago Demay - tiagoaodc@insper.edu.br\n    # Define required providers\nterraform {\n    required_version = \"&gt;= 0.14.0\"\n    required_providers {\n        openstack = {\n        source  = \"terraform-provider-openstack/openstack\"\n        version = \"~&gt; 1.35.0\"\n        }\n    }\n}\n# Configure the OpenStack Provider\n\nprovider \"openstack\" {\nregion              = \"RegionOne\"\nuser_name           = \"SEU_USUARIO\"\n}\n</code></pre></p> <p>instance1.tf <pre><code>resource \"openstack_compute_instance_v2\" \"instancia\" {\n  name            = \"basic\"\n  image_name      = \"bionic-amd64\"\n  flavor_name     = \"m1.small\"\n  key_pair        = \"mykey\"\n  security_groups = [\"default\"]\n\n  network {\n    name = \"network_1\"\n  }\n\n  depends_on = [openstack_networking_network_v2.network_1]\n\n}\n</code></pre></p> <p>instance2.tf <pre><code>resource \"openstack_compute_instance_v2\" \"instancia\" {\n  name            = \"basic2\"\n  image_name      = \"jammy-amd64\"\n  flavor_name     = \"m1.tiny\"\n  key_pair        = \"mykey\"\n  security_groups = [\"default\"]\n\n  network {\n    name = \"network_1\"\n  }\n\n  depends_on = [openstack_networking_network_v2.network_1]\n\n}\n</code></pre></p> <p>network.tf <pre><code>resource \"openstack_networking_network_v2\" \"network_1\" {\n  name           = \"network_1\"\n  admin_state_up = \"true\"\n}\n\nresource \"openstack_networking_subnet_v2\" \"subnet_1\" {\n  network_id = \"${openstack_networking_network_v2.network_1.id}\"\n  cidr       = \"192.167.199.0/24\"\n}\n</code></pre></p> <p>router.tf <pre><code>resource \"openstack_networking_router_v2\" \"router_1\" {\n  name                = \"my_router\"\n  admin_state_up      = true\n  external_network_id = &lt;\"ID_EXT_NETWORK\"&gt;\n}\n\nresource \"openstack_networking_router_interface_v2\" \"int_1\" {\n  router_id = \"${openstack_networking_router_v2.router_1.id}\"\n  subnet_id = \"${openstack_networking_subnet_v2.subnet_1.id}\"\n}\n</code></pre></p> <p>Nos arquivos acima, foi feito as altera\u00e7\u00f5es necess\u00e1rias para cada aluno, foi criado uma key_par para cada aluno e diferentes redes.</p> <p>Para fazer a implementa\u00e7\u00e3o da infraestrutura, execute os comandos abaixo:</p> <pre><code>terraform plan\n</code></pre> <p>Este comando \u00e9 utilizado para criar um plano de execu\u00e7\u00e3o. Ele mostra quais a\u00e7\u00f5es o Terraform executar\u00e1 quando voc\u00ea aplicar suas configura\u00e7\u00f5es.</p> <pre><code>terraform apply\n</code></pre> <p>O comando terraform apply aplica as mudan\u00e7as necess\u00e1rias para alcan\u00e7ar o estado desejado da sua configura\u00e7\u00e3o. Ele cria, atualiza ou destr\u00f3i os recursos conforme necess\u00e1rio.</p>"},{"location":"roteiro4/main/#tarefa-ambos","title":"Tarefa (Ambos)","text":"<p>AlunosDomain Users</p>"},{"location":"roteiro4/main/#tarefa-joao","title":"Tarefa (Jo\u00e3o)","text":"<p>Overview Instancias Network</p>"},{"location":"roteiro4/main/#tarefa-marinna","title":"Tarefa (Marinna)","text":"<p>Overview Instancias Network</p>"},{"location":"roteiro4/main/#criando-um-plano-de-disaster-recovery-e-sla","title":"Criando um plano de Disaster Recovery e SLA","text":"<ol> <li>Public Cloud ou Private Cloud?</li> <li> <p>Private Cloud. Garante isolamento f\u00edsico e l\u00f3gico para dados sens\u00edveis, controle de compliance (LGPD), criptografia dedicada e lat\u00eancia previs\u00edvel entre filiais, com custo otimizado via consolida\u00e7\u00e3o e automa\u00e7\u00e3o.</p> </li> <li> <p>Por que precisamos de um time de DevOps?</p> </li> <li> <p>CI/CD: automatiza build, testes e deploy, reduzindo lead time e risco de falhas em produ\u00e7\u00e3o.</p> </li> <li> <p>IaC: infraestrutura versionada e reproduz\u00edvel (Terraform/Ansible), elimina drift entre ambientes.</p> </li> <li> <p>Observabilidade: m\u00e9tricas e alertas proativos para detectar e corrigir incidentes antes que impactem o usu\u00e1rio.</p> </li> <li> <p>Cultura colaborativa: elimina silos Dev \u00d7 Ops, acelera entregas e resposta a mudan\u00e7as.</p> </li> <li> <p>Seguran\u00e7a integrada: scanners de vulnerabilidade e policies no pipeline, garantindo compliance cont\u00ednuo.</p> </li> <li> <p>Plano de DR &amp; HA</p> </li> </ol> <p>Amea\u00e7as principais: 1. Falha de hardware 2. Queda de energia 3. Desastre natural 4. Ataque cibern\u00e9tico (DDoS, ransomware) 5. Erro humano 6. Falha de link de rede</p> <p>Recupera\u00e7\u00e3o (priorizada) 1. Failover autom\u00e1tico para site secund\u00e1rio 2. Restore de backup cr\u00edtico em at\u00e9 1 h 3. Roll-back de vers\u00e3o pelo CI/CD 4. Execu\u00e7\u00e3o de runbooks testados 5. Mitiga\u00e7\u00e3o DDoS e bloqueio de IP malicioso</p> <p>Pol\u00edtica de backup - Incremental di\u00e1rio (ret\u00eam 7 dias) - Full semanal (4 semanas) e mensal (12 meses) - Armazenamento off-site cifrado - Testes de restore trimestrais</p> <p>Alta Disponibilidade - Clusters ativos em dois datacenters - Load balancer distribu\u00eddo - Replica\u00e7\u00e3o s\u00edncrona de banco (PostgreSQL/MySQL) - Rede redundante e BGP - Orquestra\u00e7\u00e3o de containers (Kubernetes) com anti-affinity</p> <p>Assim, cobrimos isolamento, agilidade, seguran\u00e7a e resili\u00eancia para manter o sistema cr\u00edtico sempre dispon\u00edvel e protegido.</p>"}]}